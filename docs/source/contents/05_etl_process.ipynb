{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf39aa0",
   "metadata": {},
   "source": [
    "# ETL Process (Airflow DAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09d16e1",
   "metadata": {},
   "source": [
    "Create an Airflow DAG that will daily add data from the previous day to the analytical database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1d9d1f",
   "metadata": {},
   "source": [
    "To track changes in dimensions, we will use Slowly Changing Dimension Type 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb28fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from airflow.sdk import dag, task\n",
    "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Union\n",
    "from sqlalchemy import text\n",
    "    \n",
    "# Constants\n",
    "SCHEMA_NAME = 'analytics'\n",
    "DIM_CUSTOMERS_TABLE = 'dim_customers'\n",
    "DIM_PRODUCTS_TABLE = 'dim_products'\n",
    "\n",
    "# SCD Configuration for Type 2 dimensions\n",
    "SCD_CONFIG = {\n",
    "    'customers': {\n",
    "        'id_column': 'customer_id',\n",
    "        'change_columns': ['customer_name', 'customer_category_name', 'city_name', 'state_province_name'],\n",
    "        'target_table': DIM_CUSTOMERS_TABLE\n",
    "    },\n",
    "    'products': {\n",
    "        'id_column': 'stock_item_id',\n",
    "        'change_columns': ['stock_item_name', 'stock_group_names', 'color_name',\n",
    "                          'unit_package_type_name', 'outer_package_type_name', 'brand', 'size'],\n",
    "        'target_table': DIM_PRODUCTS_TABLE\n",
    "    }\n",
    "}\n",
    "\n",
    "# Database connections\n",
    "SRC_DB_ID = 'neon_wwi'\n",
    "DST_DB_ID = 'neon_analytics'\n",
    "src_hook = PostgresHook(postgres_conn_id=SRC_DB_ID)\n",
    "dst_hook = PostgresHook(postgres_conn_id=DST_DB_ID)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'analytics_team',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2025, 9, 25),\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "}\n",
    "\n",
    "dag_config = {\n",
    "    'default_args': default_args,\n",
    "    'description': 'Daily ETL pipeline for loading data from operational DB to analytics warehouse',\n",
    "    'schedule': '0 3 * * *', # Runs at 3 AM daily\n",
    "    'catchup': False,\n",
    "    'tags': ['analytics', 'etl'],\n",
    "    'max_active_runs': 1,\n",
    "    'doc_md': \"\"\"\\\n",
    "    # Analytics Daily ETL Pipeline\n",
    "\n",
    "    This DAG performs daily incremental load from operational database (WWI) to analytics data warehouse.\n",
    "\n",
    "    ## Main Tasks:\n",
    "    - Extract daily orders, invoices, and order lines\n",
    "    - Process SCD Type 2 for customer and product dimensions\n",
    "    - Load fact and dimension tables\n",
    "\n",
    "    ## Dependencies:\n",
    "    - Requires Postgres connections: `neon_wwi` and `neon_analytics`\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "QUERY_EXTRACT_ORDERS = text(\"\"\"\n",
    "    SELECT\n",
    "        order_id\n",
    "        , customer_id\n",
    "        , order_date\n",
    "        , expected_delivery_date\n",
    "        , picking_completed_when\n",
    "    FROM\n",
    "        sales.orders\n",
    "    WHERE\n",
    "        order_date = :yesterday_ds\n",
    "\"\"\")\n",
    "\n",
    "QUERY_EXTRACT_INVOICES = text(\"\"\"\n",
    "    WITH invoice_totals AS (\n",
    "        SELECT\n",
    "            i.invoice_id\n",
    "            , SUM(il.line_profit) AS invoice_profit\n",
    "            , SUM(il.extended_price) AS invoice_amount\n",
    "            , COUNT(il.invoice_line_id) AS invoice_lines_count\n",
    "        FROM\n",
    "            sales.invoices i\n",
    "        LEFT JOIN sales.invoice_lines il ON i.invoice_id = il.invoice_id\n",
    "        WHERE\n",
    "            i.invoice_date = :yesterday_ds\n",
    "        GROUP BY\n",
    "            i.invoice_id\n",
    "    ),\n",
    "    payment_totals AS (\n",
    "        SELECT\n",
    "            invoice_id\n",
    "            , SUM(transaction_amount) AS paid_amount\n",
    "        FROM\n",
    "            sales.customer_transactions\n",
    "        WHERE\n",
    "            is_finalized = TRUE\n",
    "            AND invoice_id IN (\n",
    "                SELECT invoice_id\n",
    "                FROM sales.invoices\n",
    "                WHERE invoice_date = :yesterday_ds\n",
    "        )\n",
    "        GROUP BY\n",
    "            invoice_id\n",
    "    )\n",
    "    SELECT\n",
    "        i.invoice_id\n",
    "        , i.order_id\n",
    "        , COALESCE(pt.paid_amount, 0) AS paid_amount\n",
    "        , i.invoice_date\n",
    "        , i.delivery_method_id\n",
    "        , i.confirmed_delivery_time\n",
    "        , i.returned_delivery_data\n",
    "        , CASE WHEN i.confirmed_delivery_time IS NOT NULL THEN TRUE ELSE FALSE END AS is_delivered\n",
    "        , COALESCE(it.invoice_profit, 0) AS invoice_profit\n",
    "        , COALESCE(it.invoice_amount, 0) AS invoice_amount\n",
    "        , COALESCE(it.invoice_lines_count, 0) AS invoice_lines_count\n",
    "    FROM\n",
    "        sales.invoices i\n",
    "    LEFT JOIN invoice_totals it ON i.invoice_id = it.invoice_id\n",
    "    LEFT JOIN payment_totals pt ON i.invoice_id = pt.invoice_id\n",
    "    WHERE\n",
    "        i.invoice_date = :yesterday_ds\n",
    "\"\"\")\n",
    "\n",
    "QUERY_EXTRACT_ORDER_LINES = text(\"\"\"\n",
    "    SELECT\n",
    "        ol.order_line_id\n",
    "        , o.order_id\n",
    "        , ol.stock_item_id\n",
    "        , ol.quantity\n",
    "        , ol.unit_price\n",
    "        , il.quantity AS il_quantity\n",
    "        , il.line_profit AS il_line_profit\n",
    "        , il.extended_price AS il_extended_price\n",
    "    FROM\n",
    "        sales.orders o\n",
    "        LEFT JOIN sales.order_lines ol ON o.order_id = ol.order_id\n",
    "        LEFT JOIN sales.invoices i ON i.order_id = o.order_id\n",
    "        LEFT JOIN sales.invoice_lines il ON i.invoice_id = il.invoice_id AND ol.stock_item_id = il.stock_item_id\n",
    "    WHERE\n",
    "        o.order_date = :yesterday_ds\n",
    "\"\"\")\n",
    "\n",
    "QUERY_EXTRACT_CUSTOMERS_SRC = \"\"\"\n",
    "    SELECT\n",
    "        c.customer_id\n",
    "        , c.customer_name\n",
    "        , cc.customer_category_name\n",
    "        , ci.city_name\n",
    "        , sp.state_province_name\n",
    "    FROM\n",
    "        sales.customers c\n",
    "        LEFT JOIN sales.customer_categories cc\n",
    "            ON c.customer_category_id = cc.customer_category_id\n",
    "        LEFT JOIN application.cities ci\n",
    "            ON c.delivery_city_id = ci.city_id\n",
    "        LEFT JOIN application.state_provinces sp\n",
    "            ON ci.state_province_id = sp.state_province_id         \n",
    "\"\"\"\n",
    "\n",
    "QUERY_EXTRACT_CUSTOMERS_DST = \"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM\n",
    "        analytics.dim_customers\n",
    "    WHERE\n",
    "        current_record = TRUE\n",
    "\"\"\"\n",
    "\n",
    "QUERY_EXTRACT_PRODUCTS_SRC = \"\"\"\n",
    "    WITH stock_groups_agg AS (\n",
    "        SELECT\n",
    "            sisg.stock_item_id\n",
    "            , STRING_AGG(sg.stock_group_name, ', ') AS stock_group_names\n",
    "        FROM\n",
    "            warehouse.stock_item_stock_groups sisg\n",
    "            LEFT JOIN warehouse.stock_groups sg\n",
    "                ON sisg.stock_group_id = sg.stock_group_id\n",
    "        GROUP\n",
    "            BY sisg.stock_item_id\n",
    "    )\n",
    "    SELECT\n",
    "        si.stock_item_id\n",
    "        , si.stock_item_name\n",
    "        , sg.stock_group_names\n",
    "        , c.color_name\n",
    "        , ptu.package_type_name AS unit_package_type_name\n",
    "        , pto.package_type_name AS outer_package_type_name\n",
    "        , si.brand\n",
    "        , si.size\n",
    "    FROM\n",
    "        warehouse.stock_items si\n",
    "        LEFT JOIN stock_groups_agg sg\n",
    "            ON si.stock_item_id = sg.stock_item_id\n",
    "        LEFT JOIN warehouse.package_types ptu\n",
    "            ON si.unit_package_id = ptu.package_type_id\n",
    "        LEFT JOIN warehouse.package_types pto\n",
    "            ON si.outer_package_id = pto.package_type_id\n",
    "        LEFT JOIN warehouse.colors c\n",
    "            ON c.color_id = si.color_id\n",
    "\"\"\"\n",
    "\n",
    "QUERY_EXTRACT_PRODUCTS_DST = \"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM\n",
    "        analytics.dim_products\n",
    "    WHERE\n",
    "        current_record = TRUE\n",
    "\"\"\"\n",
    "\n",
    "def handle_etl_failure(context):\n",
    "    \"\"\"Enhanced error handling for ETL tasks\"\"\"\n",
    "    task_instance = context['task_instance']\n",
    "    exception = context.get('exception')\n",
    "    execution_date = context['execution_date']\n",
    "\n",
    "    logger.error(f\"ETL Task {task_instance.task_id} failed on {execution_date}\")\n",
    "    logger.error(f\"Exception: {str(exception)}\")\n",
    "    logger.error(f\"Task try number: {task_instance.try_number}\")\n",
    "\n",
    "    # Send email notification (uses Airflow's default email config)\n",
    "    try:\n",
    "        task_instance.email_on_failure(subject=f\"ETL Failure: {task_instance.task_id}\", html_content=None)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to send failure email: {e}\")\n",
    "\n",
    "def safe_column_comparison(col1: pd.Series, col2: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Safe column comparison handling NULL values appropriately\n",
    "    Returns boolean series indicating where values differ\n",
    "    \"\"\"\n",
    "    return (col1 != col2) & ~(col1.isna() & col2.isna())\n",
    "\n",
    "def detect_scd_type2_changes(source_df: pd.DataFrame, target_df: pd.DataFrame, entity_type: str) -> Union[pd.DataFrame, None]:\n",
    "    \"\"\"Detect changes for SCD Type 2 dimension processing\"\"\"\n",
    "    if source_df.empty:\n",
    "        logger.info(\"Source DataFrame is empty - no changes to process\")\n",
    "        return None\n",
    "\n",
    "    config = SCD_CONFIG[entity_type]\n",
    "    id_col = config['id_column']\n",
    "    change_cols = config['change_columns']\n",
    "\n",
    "    # Check duplicates\n",
    "    if source_df.duplicated(subset=[id_col]).any():\n",
    "        logger.warning(f\"Duplicate IDs found in source data for {entity_type}\")\n",
    "\n",
    "    source_df.rename(columns={id_col: f'{id_col}_src'}, inplace=True)\n",
    "    target_df.rename(columns={id_col: f'{id_col}_dst'}, inplace=True)\n",
    "\n",
    "    # Merge data\n",
    "    merged = source_df.merge(\n",
    "        target_df,\n",
    "        left_on=f'{id_col}_src',\n",
    "        right_on=f'{id_col}_dst',\n",
    "        suffixes=['_src', '_dst'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Build conditions for changes\n",
    "    change_conditions = [merged[f'{id_col}_dst'].isna()]\n",
    "    for col in change_cols:\n",
    "        change_conditions.append(\n",
    "            safe_column_comparison(merged[f'{col}_src'], merged[f'{col}_dst'])\n",
    "        )\n",
    "\n",
    "    # Combine conditions\n",
    "    changes_mask = np.any(change_conditions, axis=0)\n",
    "    changes = merged[changes_mask]\n",
    "\n",
    "    if changes.empty:\n",
    "        logger.info(f\"No dimension changes detected for {id_col}\")\n",
    "        return None\n",
    "\n",
    "    # Prepare result\n",
    "    result_columns = {f'{id_col}_src': id_col}\n",
    "    for col in change_cols:\n",
    "        result_columns[f'{col}_src'] = col\n",
    "\n",
    "    result = changes[list(result_columns.keys())].rename(columns=result_columns)\n",
    "\n",
    "    # Add SCD metadata\n",
    "    result['valid_from'] = datetime.now().date()\n",
    "    result['valid_to'] = None\n",
    "    result['current_record'] = True\n",
    "    logger.info(f\"Detected {len(result)} changes for {id_col}\")\n",
    "    return result\n",
    "\n",
    "def validate_scd_data(df: pd.DataFrame, entity_type: str) -> bool:\n",
    "    \"\"\"Validate dimension data before loading\"\"\"\n",
    "    if df is None:\n",
    "        return True\n",
    "\n",
    "    config = SCD_CONFIG[entity_type]\n",
    "    id_col = config['id_column']\n",
    "\n",
    "    # Check required columns\n",
    "    required_columns = [id_col, 'valid_from', 'valid_to', 'current_record']\n",
    "    missing_columns = set(required_columns) - set(df.columns)\n",
    "    if missing_columns:\n",
    "        logger.error(f\"Missing required columns: {missing_columns}\")\n",
    "        return False\n",
    "\n",
    "    # Validate primary key\n",
    "    if df[id_col].isna().any():\n",
    "        logger.error(f\"NULL values found in primary key column: {id_col}\")\n",
    "        return False\n",
    "\n",
    "    # Check for duplicates\n",
    "    duplicate_ids = df[df.duplicated(subset=[id_col])]\n",
    "    if not duplicate_ids.empty:\n",
    "        logger.warning(f\"Found {len(duplicate_ids)} duplicate IDs in source data\")\n",
    "\n",
    "    return True\n",
    "\n",
    "def load_scd_type_2(changed_data: pd.DataFrame, entity_type: str) -> None:\n",
    "    \"\"\"Load dimension changes using SCD Type 2 approach\"\"\"\n",
    "    if changed_data is None or changed_data.empty:\n",
    "        logger.info(f\"No changes to load for {entity_type}\")\n",
    "        return\n",
    "\n",
    "    config = SCD_CONFIG[entity_type]\n",
    "    id_col = config['id_column']\n",
    "    table_name = f\"dim_{entity_type}\"\n",
    "\n",
    "    engine = dst_hook.get_sqlalchemy_engine()\n",
    "    try:\n",
    "        with engine.begin() as connection:\n",
    "            # Update existing records\n",
    "            ids = changed_data[id_col].tolist()\n",
    "            if ids:\n",
    "                update_sql = text(f\"\"\"\n",
    "                    UPDATE {SCHEMA_NAME}.{table_name}\n",
    "                    SET\n",
    "                        valid_to = CURRENT_DATE - INTERVAL '1 day'\n",
    "                        , current_record = FALSE\n",
    "                    WHERE\n",
    "                        {id_col} = ANY(:ids)\n",
    "                        AND current_record = TRUE\n",
    "                \"\"\")\n",
    "                \n",
    "                connection.execute(update_sql, {'ids': ids})\n",
    "                # Insert new records\n",
    "                changed_data.to_sql(\n",
    "                    table_name,\n",
    "                    connection,\n",
    "                    schema=SCHEMA_NAME,\n",
    "                    if_exists='append',\n",
    "                    index=False,\n",
    "                    method='multi'\n",
    "                )\n",
    "\n",
    "                logger.info(f\"Successfully loaded {len(ids)} new records for {entity_type}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load {entity_type} dimension: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "@dag(**dag_config)\n",
    "def wwi_to_analytics_daily():\n",
    "\n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def extract_orders(**context) -> pd.DataFrame:\n",
    "        \"\"\"Extracts daily orders data from source system for incremental processing.\"\"\"\n",
    "        logical_date = context['logical_date']\n",
    "        yesterday_ds = (logical_date - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        result = src_hook.get_pandas_df(QUERY_EXTRACT_ORDERS, parameters={'yesterday_ds': yesterday_ds})\n",
    "        return result\n",
    "\n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def extract_invoices(**context) -> pd.DataFrame:\n",
    "        \"\"\"Extracts invoice information with calculated totals and payment status for the processing date.\"\"\"\n",
    "        logical_date = context['logical_date']\n",
    "        yesterday_ds = (logical_date - timedelta(days=1)).strftime('%Y-%m-%d')        \n",
    "        result = src_hook.get_pandas_df(QUERY_EXTRACT_INVOICES, parameters={'yesterday_ds': yesterday_ds})\n",
    "        return result\n",
    "\n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def extract_order_lines(**context) -> pd.DataFrame:\n",
    "        \"\"\"Extracts order line details with associated invoice information for daily sales analysis.\"\"\"\n",
    "        logical_date = context['logical_date']\n",
    "        yesterday_ds = (logical_date - timedelta(days=1)).strftime('%Y-%m-%d')           \n",
    "        result = src_hook.get_pandas_df(QUERY_EXTRACT_ORDER_LINES, parameters={'yesterday_ds': yesterday_ds})\n",
    "        return result\n",
    "\n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def extract_customers_src() -> pd.DataFrame:\n",
    "        \"\"\"Extracts current customer master data including categories and geographical information from source.\"\"\"       \n",
    "        result = src_hook.get_pandas_df(QUERY_EXTRACT_CUSTOMERS_SRC)\n",
    "        return result\n",
    "\n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def extract_customers_dst() -> pd.DataFrame:\n",
    "        \"\"\"Extracts existing customer dimension records from analytics warehouse for change detection.\"\"\"\n",
    "        result = dst_hook.get_pandas_df(QUERY_EXTRACT_CUSTOMERS_DST)\n",
    "        return result\n",
    "\n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def extract_products_src() -> pd.DataFrame:\n",
    "        \"\"\"Extracts product master data with attributes and classifications from operational database.\"\"\"\n",
    "        result = src_hook.get_pandas_df(QUERY_EXTRACT_PRODUCTS_SRC)\n",
    "        return result\n",
    "\n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def extract_products_dst() -> pd.DataFrame:\n",
    "        \"\"\"Extracts current product dimension records from data warehouse for SCD comparison.\"\"\"\n",
    "        result = dst_hook.get_pandas_df(QUERY_EXTRACT_PRODUCTS_DST)\n",
    "        return result\n",
    "\n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def validate_source_data(df: pd.DataFrame, table_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Basic data validation\"\"\"\n",
    "        if df.empty:\n",
    "            logger.error(f\"Empty DataFrame for {table_name}\")\n",
    "            raise ValueError(f\"Empty data for {table_name}\")\n",
    "\n",
    "        if df.isnull().any().any():\n",
    "            null_cols = df.columns[df.isnull().any()].tolist()\n",
    "            logger.warning(f\"NULL values found in columns: {null_cols}\")\n",
    "\n",
    "        if df.duplicated().any():\n",
    "            logger.warning(f\"Found {df.duplicated().sum()} duplicates in {table_name}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def transform_check_changed_customers(customers_src: pd.DataFrame, customers_dst: pd.DataFrame) -> Union[pd.DataFrame, None]:\n",
    "        \"\"\"Identifies customer dimension changes using SCD Type 2 logic for incremental updates.\"\"\"\n",
    "        return detect_scd_type2_changes(customers_src, customers_dst, 'customers')\n",
    "\n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def transform_check_changed_products(products_src: pd.DataFrame, products_dst: pd.DataFrame) -> Union[pd.DataFrame, None]:\n",
    "        \"\"\"Identifies product dimension modifications requiring new version creation in the warehouse.\"\"\"\n",
    "        return detect_scd_type2_changes(products_src, products_dst, 'products')\n",
    "\n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def load_fact_orders(df_orders: pd.DataFrame) -> None:\n",
    "        \"\"\"Load orders fact table\"\"\"\n",
    "        if df_orders.empty:\n",
    "            logger.info(\"No orders data to load\")\n",
    "            return\n",
    "        try:\n",
    "            engine = dst_hook.get_sqlalchemy_engine()\n",
    "            df_orders.to_sql(\n",
    "                'fact_orders',\n",
    "                engine,\n",
    "                schema=SCHEMA_NAME,\n",
    "                if_exists='append',\n",
    "                index=False,\n",
    "                method='multi'\n",
    "            )\n",
    "            logger.info(f\"Loaded {len(df_orders)} orders\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load orders: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def load_fact_invoices(df_invoices: pd.DataFrame) -> None:\n",
    "        \"\"\"Load invoices fact table\"\"\"\n",
    "        if df_invoices.empty:\n",
    "            logger.info(\"No invoices data to load\")\n",
    "            return\n",
    "        try:\n",
    "            engine = dst_hook.get_sqlalchemy_engine()\n",
    "            df_invoices.to_sql(\n",
    "                'fact_invoices',\n",
    "                engine,\n",
    "                schema=SCHEMA_NAME,\n",
    "                if_exists='append',\n",
    "                index=False,\n",
    "                method='multi'\n",
    "            )\n",
    "            logger.info(f\"Loaded {len(df_invoices)} invoices\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load invoices: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def load_fact_order_lines(df_order_lines: pd.DataFrame) -> None:\n",
    "        \"\"\"Load order lines fact table\"\"\"\n",
    "        if df_order_lines.empty:\n",
    "            logger.info(\"No order lines data to load\")\n",
    "            return\n",
    "        try:\n",
    "            engine = dst_hook.get_sqlalchemy_engine()\n",
    "            df_order_lines.to_sql(\n",
    "                'fact_order_lines',\n",
    "                engine,\n",
    "                schema=SCHEMA_NAME,\n",
    "                if_exists='append',\n",
    "                index=False,\n",
    "                method='multi'\n",
    "            )\n",
    "            logger.info(f\"Loaded {len(df_order_lines)} order lines\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load order lines: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def load_scd_type_2_customers(changed_customers: pd.DataFrame) -> None:\n",
    "        \"\"\"Loads customer dimension changes by closing old records and inserting new versions with validation.\"\"\"\n",
    "        if validate_scd_data(changed_customers, 'customers'):\n",
    "            load_scd_type_2(changed_customers, 'customers')\n",
    "\n",
    "    @task(\n",
    "        retries=3,\n",
    "        retry_delay=timedelta(minutes=5),\n",
    "        on_failure_callback=handle_etl_failure\n",
    "    )\n",
    "    def load_scd_type_2_products(changed_products: pd.DataFrame) -> None:\n",
    "        \"\"\"Applies product dimension updates using SCD Type 2 pattern with data quality checks.\"\"\"\n",
    "        if validate_scd_data(changed_products, 'products'):\n",
    "            load_scd_type_2(changed_products, 'products')\n",
    "\n",
    "    # ==========================================================================\n",
    "    # WORKFLOW\n",
    "    # ==========================================================================\n",
    "\n",
    "    # Extract\n",
    "    df_orders = extract_orders()\n",
    "    df_invoices = extract_invoices()\n",
    "    df_order_lines = extract_order_lines()\n",
    "\n",
    "    df_customers_src = extract_customers_src()\n",
    "    df_customers_dst = extract_customers_dst()\n",
    "    df_products_src = extract_products_src()\n",
    "    df_products_dst = extract_products_dst()\n",
    "\n",
    "    # Validate\n",
    "    df_orders_valid = validate_source_data(df_orders, \"orders\")\n",
    "    df_invoices_valid = validate_source_data(df_invoices, \"invoices\")\n",
    "    df_order_lines_valid = validate_source_data(df_order_lines, \"order_lines\")\n",
    "\n",
    "    # Transform\n",
    "    df_customers_changes = transform_check_changed_customers(df_customers_src, df_customers_dst)\n",
    "    df_products_changes = transform_check_changed_products(df_products_src, df_products_dst)\n",
    "\n",
    "    # Load facts\n",
    "    load_fact_orders(df_orders_valid)\n",
    "    load_fact_invoices(df_invoices_valid)\n",
    "    load_fact_order_lines(df_order_lines_valid)\n",
    "    \n",
    "    # Load dimensions\n",
    "    load_scd_type_2_customers(df_customers_changes)\n",
    "    load_scd_type_2_products(df_products_changes)\n",
    "\n",
    "# ==========================================================================\n",
    "# Instantiate the DAG\n",
    "# ==========================================================================\n",
    "\n",
    "wwi_to_analytics_daily()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
